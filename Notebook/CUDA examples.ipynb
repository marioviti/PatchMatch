{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Credits to \n",
    "https://nyu-cds.github.io/python-numba/05-cuda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel declaration\n",
    "\n",
    "A kernel function is a GPU function that is meant to be called from CPU code. It has two fundamental characteristics:\n",
    "\n",
    "*    kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array);\n",
    "*    kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel invovocation\n",
    "There are two main steps:\n",
    "\n",
    "*   Instantiate the kernel proper, by specifying a number of blocks per grid and a number of threads per block. The product of the two will give the total number of threads launched. Kernel instantiation is done by taking the compiled kernel function and indexing it with a tuple of integers.\n",
    "*    Running the kernel, by passing it the input array (and any separate output arrays if necessary). By default, running a kernel is synchronous: the function returns when the kernel has finished executing and the data is synchronized back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# Create the data array - usually initialized some other way\n",
    "data = numpy.ones(256)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# Now start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "\n",
    "# Print the result\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the block size\n",
    "\n",
    "The two-level thread hierarchy is important for the following reasons:\n",
    "\n",
    "*    On the software side, the block size determines how many threads share a given area of shared memory.\n",
    "*    On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the CUDA C Programming Guide.\n",
    "\n",
    "The block size you choose depends on a range of factors, including:\n",
    "\n",
    "*    The size of the data array\n",
    "*    The size of the shared memory per block (e.g. 64KB)\n",
    "*    The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "*    The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "*    The maximum number of blocks per MP (e.g. 32)\n",
    "*    The number of threads that can be executed concurrently (a “warp” i.e. 32)\n",
    "\n",
    "The execution of threads in a warp has a big effect on the computational throughput. If all threads in a warp are executing the same instruction then they can all be executed in parallel. But if one or more threads is executing a different instruction, the warp has to be split into groups of threads, and these groups execute serially.\n",
    "\n",
    "Rules of thumb for threads per block:\n",
    "\n",
    "*    Should be a round multiple of the warp size (32)\n",
    "*    A good place to start is 128-512 but benchmarking is required to determine the optimal value.\n",
    "\n",
    "Each streaming multiprocessor (SP) on the GPU must have enough active warps to achieve maximum throughput. In other words, the blocksize is usually selected to maximize the “occupancy”. See the CUDA (http://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls)\n",
    "Occupancy Calculator spreadsheet for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread positioning\n",
    "\n",
    "When running a kernel, the kernel function’s code is executed by every thread once. It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for. More complex algorithms may define more complex responsibilities, but the underlying principle is the same.\n",
    "\n",
    "To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids. In the example above, you could make blockspergrid and threadsperblock tuples of one, two or three integers. Compared to 1-dimensional declarations of equivalent sizes, this doesn’t change anything to the efficiency or behaviour of generated code, but can help you write your algorithms in a more natural way.\n",
    "\n",
    "One way is for the thread to determines its position in the grid and block and manually compute the corresponding array position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + ty * bw\n",
    "    if pos < io_array.size:  # Check array boundaries\n",
    "        io_array[pos] *= 2 # do the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  8.  8.  8.  8.]\n"
     ]
    }
   ],
   "source": [
    "# Now start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "\n",
    "# Print the result\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unless you are sure the block size and grid size is a divisor of your array size, you must check boundaries as shown above.\n",
    "\n",
    "The following special objects are provided by the CUDA backend for the sole purpose of knowing the geometry of the thread hierarchy and the position of the current thread within that geometry:\n",
    "\n",
    "*    numba.cuda.threadIdx - The thread indices in the current thread block. For 1-dimensional blocks, the index (given by the x attribute) is an integer spanning the range from 0 to numba.cuda.blockDim - 1. A similar rule exists for each dimension when more than one dimension is used.\n",
    "*    numba.cuda.blockDim - The shape of the block of threads, as declared when instantiating the kernel. This value is the same for all threads in a given kernel, even if they belong to different blocks (i.e. each block is “full”).\n",
    "*    numba.cuda.blockIdx - The block indices in the grid of threads launched a kernel. For a 1-dimensional grid, the index (given by the x attribute) is an integer spanning the range from 0 to numba.cuda.gridDim - 1. A similar rule exists for each dimension when more than one dimension is used.\n",
    "*    numba.cuda.gridDim - The shape of the grid of blocks, i.e. the total number of blocks launched by this kernel invocation, as declared when instantiating the kernel.\n",
    "\n",
    "These objects can be 1-, 2- or 3-dimensional, depending on how the kernel was invoked. To access the value at each dimension, use the x, y and z attributes of these objects, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute positions\n",
    "\n",
    "Simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
    "\n",
    "*    numba.cuda.grid(ndim) - Return the absolute position of the current thread in the entire grid of blocks. ndim should correspond to the number of dimensions declared when instantiating the kernel. If ndim is 1, a single integer is returned. If ndim is 2 or 3, a tuple of the given number of integers is returned.\n",
    "*    numba.cuda.gridsize(ndim) - Return the absolute size (or shape) in threads of the entire grid of blocks. ndim has the same meaning as in grid() above.\n",
    "\n",
    "Using these functions, the our example can become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel2(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2 # do the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory management\n",
    "\n",
    "Numba has been automatically transferring the NumPy arrays to the device when you invoke the kernel. However, it can only do so conservatively by always transferring the device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, it is possible to manually control the transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocates empty device ndarray\n",
    "shape = (int(10e6))\n",
    "device_array = cuda.device_array( shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate and transfer a NumPy ndarray to the device.\n",
    "array = np.ones(int(10e6))\n",
    "device_array = cuda.to_device( array )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "The following code sample is a straightforward implementation of matrix multiplication for matrices where each thread reads one row of A and one column of B and computes the corresponding element of C. For input arrays where A.shape == (m, n) and B.shape == (n, p) then the result shape will be C.shape = (m, p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]\n",
      " [ 144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.  144.\n",
      "   144.  144.  144.  144.  144.  144.  144.  144.  144.  144.]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = numpy.full((24, 12), 3, numpy.float) # matrix containing all 3's\n",
    "B = numpy.full((12, 22), 4, numpy.float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((24, 22))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with this code is that each thread is reading from the global memory containing the copies of A and B. In fact, the global memory is read B.shape[1] times and A.shape[0] times. Since Global memory is fairly slow, this results in an inefficient use of GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared memory and thread synchronization\n",
    "\n",
    "A limited amount of shared memory can be allocated on the device to speed up access to data, when necessary. That memory will be shared (i.e. both readable and writable) amongst all threads belonging to a given block and has faster access times than regular device memory. It also allows threads to cooperate on a given solution. You can think of it as a manually-managed data cache.\n",
    "\n",
    "The function to create a shared memory array is:\n",
    "\n",
    "`shared_array = cuda.shared.array(shape,type)\n",
    "`\n",
    "* shape:\n",
    "is either an integer or a tuple of integers representing the array’s dimensions.\n",
    "* type:\n",
    "is a Numba type of the elements needing to be stored in the array.\n",
    "\n",
    "The memory is allocated once for the duration of the kernel, unlike traditional dynamic memory management.\n",
    "Because the shared memory is a limited resource, it is often necessary to preload a small block at a time from the input arrays. All the threads then need to wait until everyone has finished preloading before doing the computation on the shared memory.\n",
    "\n",
    "Synchronization is then required again after the computation to ensure all threads have finished with the data in shared memory before overwriting it in the next loop iteration.\n",
    "\n",
    "The function to synchronized threads is:\n",
    "\n",
    "`\n",
    "cuda.syncthreads()\n",
    "`\n",
    "\n",
    "This function will synchronize all threads in the same thread block. This function implements the same pattern as barriers in traditional multi-threaded programming and the MPI.Barrier() function. The program will wait until all threads in the block call the function, at which point it returns control to all its callers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved matrix multiplication\n",
    "\n",
    "The following example shows how shared memory can be used when performing matrix multiplication.\n",
    "\n",
    "In this example, each thread block is responsible for computing a square sub-matrix of C and each thread for computing an element of the sub-matrix. The sub-matrix is equal to the product of a square sub-matrix of A (sA) and a square sub-matrix of B (sB). In order to fit into the device resources, the two input matrices are divided into as many square sub-matrices of dimension TPB as necessary, and the result computed as the sum of the products of these square sub-matrices.\n",
    "\n",
    "Each product is performed by first loading sA and sB from global memory to shared memory, with one thread loading each element of each sub-matrix. Once sA and sB have been loaded, each thread accumulates the result into a register (tmp). Once all the products have been calculated, the results are written to the matrix C in global memory.\n",
    "\n",
    "By blocking the computation this way, we can reduce the number of global memory accesses since A is now only read B.shape[1] / TPB times and B is read A.shape[0] / TPB times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockspergrid(2, 2)\n",
      "threadsperblock(2, 2)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda, float32\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 2\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "\n",
    "# The data array\n",
    "A = numpy.full((TPB*2, TPB*100000), 3, numpy.float) # [4 x 16] matrix containing all 3's\n",
    "B = numpy.full((TPB*100000, TPB*2), 4, numpy.float) # [16 x 4] matrix containing all 4's\n",
    "\n",
    "#print(\"A: {}\\n\".format(A))\n",
    "#print(\"B: {}\\n\".format(B))\n",
    "\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "C_global_mem = cuda.device_array((TPB*2, TPB*2)) # [4 x 4] matrix result\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = int(np.ceil(A.shape[0] / threadsperblock[1]))\n",
    "blockspergrid_y = int(np.ceil(B.shape[1] / threadsperblock[0]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "print('blockspergrid{}'.format(blockspergrid))\n",
    "print('threadsperblock{}'.format(threadsperblock))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n",
      "[[ 2400000.  2400000.  2400000.  2400000.]\n",
      " [ 2400000.  2400000.  2400000.  2400000.]\n",
      " [ 2400000.  2400000.  2400000.  2400000.]\n",
      " [ 2400000.  2400000.  2400000.  2400000.]]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Start the kernel \n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "res = C_global_mem.copy_to_host()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2400000.,  2400000.,  2400000.,  2400000.],\n",
       "       [ 2400000.,  2400000.,  2400000.,  2400000.],\n",
       "       [ 2400000.,  2400000.,  2400000.,  2400000.],\n",
       "       [ 2400000.,  2400000.,  2400000.,  2400000.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "np.matmul(A,B)\n",
    "# whatever numpy is doing is awsome..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronoi with Jump Flood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Seeds_Position = [(50,50), (100,250), (300,200), (169,169), (10,40)]\n",
    "\n",
    "Seed_number = len(Seeds_Position)\n",
    "A = np.zeros((320,320)) -1\n",
    "for i in range(len(Seeds_Position)):\n",
    "    A[Seeds_Position[i]] = i\n",
    "\n",
    "offsets = [ (-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1),(0,0) ]\n",
    "offsets_number = len(offsets)\n",
    "\n",
    "@cuda.jit\n",
    "def Voronoi(A, B, Seeds_Position, offsets, k):\n",
    "    cellx, celly = cuda.grid(2)\n",
    "    \n",
    "    soffsets= cuda.shared.array(shape=(offsets_number, 2), dtype=float32)\n",
    "    sSeeds_Position = cuda.shared.array(shape=(Seed_number, 2), dtype=float32)\n",
    "    \n",
    "    # first thread of each block copies Seeds_Position and offsets to shared memory\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    if tx == 0 and ty == 0:\n",
    "        cuda.syncthreads()\n",
    "        for i in range(offsets_number):\n",
    "            soffsets[i,0] = offsets[i,0]\n",
    "            soffsets[i,1] = offsets[i,1]\n",
    "        for i in range(Seed_number):\n",
    "            sSeeds_Position[i,0] = Seeds_Position[i,0]\n",
    "            sSeeds_Position[i,1] = Seeds_Position[i,1]\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "    seed = A[cellx,celly]\n",
    "    if (seed < 0):\n",
    "        return\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        nextCellx = int(cellx + k*soffsets[i][0])\n",
    "        nextCelly = int(celly + k*soffsets[i][1])\n",
    "        \n",
    "        if nextCellx>0 and nextCellx<A.shape[0] and nextCelly>0 and nextCelly<A.shape[1]:\n",
    "            \n",
    "            # Critical Session\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            nextSeed = B[nextCellx,nextCelly]\n",
    "            if nextSeed < 0:\n",
    "                B[nextCellx,nextCelly] = seed\n",
    "            else:\n",
    "                # Read shared memory\n",
    "                seedx,seedy = sSeeds_Position[int(seed)]\n",
    "                nextSeedx,nextSeedy = sSeeds_Position[int(nextSeed)]\n",
    "                \n",
    "                # compute distance from seed and nextSeed\n",
    "                dist_seed = (nextCellx - seedx)**2 + (nextCelly - seedy)**2\n",
    "                dist_nextSeed = (nextCellx - nextSeedx)**2 + (nextCelly - nextSeedy)**2\n",
    "                if dist_seed < dist_nextSeed:\n",
    "                    B[nextCellx,nextCelly] = seed\n",
    "                    \n",
    "            cuda.syncthreads()\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 320)\n",
      "(10, 10)\n",
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n,m = A.shape       \n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(A)\n",
    "Seeds_Position_global_mem = cuda.to_device(np.array(Seeds_Position,dtype=np.int32))\n",
    "offsets_global_mem = cuda.to_device(np.array(offsets,dtype=np.int32))\n",
    "\n",
    "\n",
    "TPB = 32\n",
    "threadsperblock = (TPB,TPB)\n",
    "blockspergrid = ((n+TPB-1)//TPB, (m+TPB-1)//TPB)\n",
    "print(A.shape)\n",
    "print(blockspergrid)\n",
    "print(threadsperblock)\n",
    "\n",
    "q = 2\n",
    "k = n\n",
    "while(k>0):\n",
    "    k = n//(2**q)\n",
    "    q += 1\n",
    "    if q%2 == 0:\n",
    "        src,dst = A_global_mem,B_global_mem\n",
    "    else:\n",
    "        src,dst = B_global_mem,A_global_mem\n",
    "    Voronoi[blockspergrid, threadsperblock](src, dst,\n",
    "                                            Seeds_Position_global_mem, \n",
    "                                            offsets_global_mem,\n",
    "                                            k)\n",
    "\n",
    "res = dst.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3ebb123c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3eb740940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(A)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
